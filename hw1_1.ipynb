{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학번: 2021136128\n",
    "- 이름: 조은샘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #PyTorch 모듈 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: 딥러닝을 위한 Python 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 클래스와 메소드 이용하여 텐서 생성하는 방법\n",
    "#### * GPU 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "# 클래스 이용 텐서 생성\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu') #텐서 저장 위치 CPU로 지정\n",
    "\n",
    "# 텐서 정보 출력\n",
    "print(t1.dtype)   # 텐서 데이터 타입 >>> torch.float32\n",
    "print(t1.device)  # 텐서 저장된 장치 >>> cpu\n",
    "print(t1.requires_grad)  # 텐서의 기울기 요구 여부 >>> False\n",
    "print(t1.size())  # 텐서 크기 >>> torch.Size([3])\n",
    "print(t1.shape)   # 텐서 모양 출력 >>> torch.Size([3])\n",
    "\n",
    "# 텐서 GPU, CPU로 이동시키는 법\n",
    "# if you have gpu device\n",
    "## t1_cuda = t1.to(torch.device('cuda')) # GPU로 텐서 이동\n",
    "# or you can use shorthand\n",
    "## t1_cuda = t1.cuda() # 축약형\n",
    "t1_cpu = t1.cpu() # CPU로 텐서 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:13:33.775422Z",
     "start_time": "2023-09-05T11:13:33.751252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function\n",
    "# 메소드 이용 텐서 생성\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu') #텐서 저장 위치 CPU로 지정\n",
    "\n",
    "# 텐서 정보 출력 - 위와 텐서 데이터 타입만 다름\n",
    "print(t2.dtype)   # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # >>> torch.Size([3])\n",
    "print(t2.shape)   # >>> torch.Size([3])\n",
    "\n",
    "# 텐서 GPU, CPU로 이동시키는 법 - 위와 동일\n",
    "# if you have gpu device\n",
    "## t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "## t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 생성에 따른 텐서 모양과 차원 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n",
      "torch.Size([4, 1, 2, 2]) 4\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "# 0개의 원소를 갖는 0차원 텐서\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim) \n",
    "# 1개의 원소를 갖는 1차원 텐서\n",
    "# 1개씩 1개\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "# 5개의 원소를 갖는 1차원 텐서\n",
    "# 5개씩 1개\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)                      \n",
    "# 5개의 원소를 갖는 2차원 텐서\n",
    "# 1개씩 2개\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "# 6개의 원소를 갖는 2차원 텐서\n",
    "# 2개씩 3개\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "# 6개의 원소를 갖는 3차원 텐서\n",
    "# (1개씩 2개)가 3개\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "# 6개의 원소를 갖는 4차원 텐서\n",
    "# ((1개씩 2개)가 1개)가 3개\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "# 18개의 원소를 갖는 4차원 텐서\n",
    "# ((3개씩 2개)가 1개)가 3개\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "# 18개의 원소를 갖는 5차원 텐서\n",
    "# (((1개씩 3개)가 2개)가 1개)가 3개\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "# 20개의 원소를 갖는 2차원 텐서\n",
    "# 5개씩 4개\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "# 20개의 원소를 갖는 3차원 텐서\n",
    "# (5개씩 1개)가 4개\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "# 모양(비표준 표현) -> shape: torch.Size([4, 1, 2, 3 or 2])\n",
    "# 오류 발생: 텐서의 4 번째 축의 크기가 모두 동일하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- 텐서의 모든 축의 크기는 동일하다는 특징이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# b_tensor_initialization_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np #  NumPy 라이브러리를 np 별명으로 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpy 라이브러리: 수치 계산을 위한 라이브러리로, 다차원 배열을 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 리스트를 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T10:44:50.432010300Z",
     "start_time": "2023-09-10T10:44:47.689179700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "torch.float32\n",
      "torch.int64\n",
      "torch.int64\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "print(type(l1[0])) # + 리스트의 원소의 타입 >>> <class 'int'>\n",
    "t1 = torch.Tensor(l1) \n",
    "print(t1.dtype) # + 데이터 타입: 클래스에 의해 결정 >>> torch.float32\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2) \n",
    "print(t2.dtype) # + 데이터 타입: 인자 타입에 의해 결정 >>> torch.int64\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3) \n",
    "print(t3.dtype) # + 데이터 타입: 인자 타입에 의해 결정 >>> torch.int64\n",
    "\n",
    "# 리스트 원소 변경\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "# 텐서 출력\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "# 모든 텐서의 데이터 값이 변경되지 않음\n",
    "# 모든 변환 방법에서 리스트와 변환된 텐서는 깊은 복사 됨 (공유 안함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * numpy 배열을 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "torch.float32\n",
      "torch.int32\n",
      "torch.int32\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "print(l4.dtype) # + numpy 배열의 타입 >>> int32\n",
    "t4 = torch.Tensor(l4) \n",
    "print(t4.dtype) # + 데이터 타입: 클래스에 의해 결정 >>> torch.float32\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "print(t5.dtype) # + 데이터 타입: 클래스에 의해 결정 >>> torch.int32\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "print(t6.dtype) # + 데이터 타입: 클래스에 의해 결정 >>> torch.int32\n",
    "\n",
    "# numpy 배열 원소 변경\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "# 텐서 출력\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "# torch.as_tensor()로 numpy 배열을 텐서로 변환한 경우\n",
    "    # 이 경우만 텐서의 데이터가 동일하게 변경되었음\n",
    "    # 이 변환 방법에서 numpy 배열과 변환된 텐서는 얕은 복사 됨 (공유 함)\n",
    "# torch.Tensor()와 torch.tensor()로 변환한 경우\n",
    "    # 텐서의 데이터가 변경되지 않았음\n",
    "    # 이 두 변환 방법에서 numpy 배열과 변환된 텐서는 깊은 복사 됨 (공유 안함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# c_tensor_initialization_constant_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 특정 값을 갖는 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T10:54:51.221424200Z",
     "start_time": "2023-09-10T10:54:51.191104500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "torch.float32\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "torch.float32\n",
      "tensor([ 1.0000e+00,         nan, -3.1441e-35,  4.0778e-43])\n",
      "tensor([0.0000e+00, 6.9795e-39, 1.4013e-45, 0.0000e+00])\n",
      "torch.float32\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 지정된 크기로 텐서를 생성하고, \n",
    "\n",
    "# 모든 원소가 1인 지정된 크기의 텐서 생성 \n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "                            # 모든 원소 1이고,\n",
    "                            # 원소가 5개이고, 1차원인 텐서 생성\n",
    "t1_like = torch.ones_like(input=t1) # 입력 텐서와 같은 크기, 데이터 타입의 1로 채워진 텐서 생성\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1.dtype) # + 데이터 타입 >>> torch.float32\n",
    "\n",
    "# 모든 원소가 0인 지정된 크기의 텐서 생성 \n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "                             # 모든 원소 0이고,\n",
    "                             # 원소가 6개이고, 1차원인 텐서 생성\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2.dtype) # + 데이터 타입 >>> torch.float32\n",
    "\n",
    "# 초기화되지 않은 지정된 크기의 텐서 생성 (쓰레기 값 갖음)\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "                             # 모든 원소가 초기화 되지 않고,\n",
    "                             # 원소가 4개이고, 1차원인 텐서 생성\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3.dtype) # + 데이터 타입 >>> torch.float32\n",
    "\n",
    "# 지정된 크기의 항등 행렬 생성\n",
    "t4 = torch.eye(n=3) # 크가 3x3인 항등 행렬 생성\n",
    "print(t4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# d_tensor_initialization_random_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 램덤 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 12]])\n",
      "tensor([[0.0453, 0.5035, 0.9978]])\n",
      "tensor([[0.7419, 0.5923, 0.2908]])\n",
      "tensor([[ 9.2270, 10.0961],\n",
      "        [ 9.4067,  9.5878],\n",
      "        [10.0763, 11.1161]])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 값: 10 이상 20 미만의 랜덤 정수로 구성된 1x2 크기의 텐서 생성\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "# 0 이상 1 미만의 랜덤 값으로 구성된 1x3 크기의 텐서 생성\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "# 평균 0, 표준 편차 1을 따르는 정규 분포에서 랜덤 값으로 구성된 1x3 크기의 텐서 생성\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "# 평균 10.0, 표준 편차 1.0인 정규 분포에서 랜덤 값으로 구성된 3x2 크기의 텐서 생성\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 범위를 지정한 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 시작 값 0.0부터 종료 값 5.0까지를 균일하게 나눈 3개의 값으로 구성된 텐서 생성\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "# 0부터 4까지의 값을 가지는 1차원 텐서를 생성\n",
    "t6 = torch.arange(5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 시드와 랜덤 텐서 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T10:57:14.472374500Z",
     "start_time": "2023-09-10T10:57:14.422798900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729) # 난수 생성 시드 설정\n",
    "random1 = torch.rand(2, 3) # 랜덤 텐서 생성\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3) # 랜덤 텐서 생성\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)  # 같은 값으로 시드 설정\n",
    "random3 = torch.rand(2, 3) #랜덤 텐서 생성\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3) #랜덤 텐서 생성\n",
    "print(random4)\n",
    "\n",
    "# 시드 설정 후 랜덤 텐서를 생성할 때마다 랜덤 값이 변함\n",
    "# 한 시드에 대한 변하는 랜덤 값은 매번 동일함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 타입 변환 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 기본 타입으로 생성 (torch.float32)\n",
    "a = torch.ones((2, 3)) # 모든 원소가 1인 2x3 크기의 텐서 생성\n",
    "print(a.dtype) # >>> torch.float32\n",
    "\n",
    "# 타입 지정으로 생성 (torch.int16)\n",
    "b = torch.ones((2, 3), dtype=torch.int16)  # 모든 원소가 1이고, 데이터 타입을 int16으로 지정한 2x3 크기의 텐서 생성\n",
    "print(b)\n",
    "\n",
    "# 타입 지정으로 생성 (torch.float64)\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20. # 랜덤 값의 기본 범위는 0이상 1미만\n",
    "                                                  # 텐서의 모든 데이터에 20을 곱해지므로 랜덤 값의 범위는 0이상 20미만이 됨\n",
    "print(c)\n",
    "\n",
    "# 텐서의 데이터 타입을 int16에서 int32로 변환\n",
    "d = b.to(torch.int32)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 타입 변환 2: double, short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T10:59:22.218966700Z",
     "start_time": "2023-09-10T10:59:22.151590800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성 시에 매개변수 활용하여 타입 지정\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "# double(), short() 메소드를 사용하여 타입 변환\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "\n",
    "# to() 메소드를 사용하여 타입 변환\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "# type() 메소드를 사용하여 타입 변환\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "# 위 4가지 경우 모두 같은 타입으로 변환 됨\n",
    "print(double_d.dtype) # >>> torch.float64\n",
    "print(short_e.dtype) # >>> torch.int16\n",
    "\n",
    "# 두 텐서의 곱셈 연산 결과\n",
    "double_f = torch.rand(5, dtype=torch.double) # torch.float64 타입인 랜덤 텐서 생성\n",
    "short_g = double_f.to(torch.short) # 텐서의 타입을 torch.int16로 변환한 텐서 생성\n",
    "print((double_f * short_g).dtype) # >>> torch.float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch에서 다른 데이터 타입의 텐서를 곱셈하면, 결과의 데이터 타입은 연산에 참여한 텐서 중 더 \"높은\" 데이터 타입으로 자동 변환된다. \n",
    "- \"높은\" 데이터 타입: 더 큰 범위의 숫자를 나타낼 수 있는 데이터 타입을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 크기가 같은 두개의 텐서 생성\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 연산: 덧셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 덧셈 연산하여 저장\n",
    "t3 = torch.add(t1, t2) # 메소드 이용\n",
    "t4 = t1 + t2 # 연산자 이용\n",
    "\n",
    "# 결과 출력\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐세 연산: 뺄셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 뺄셈 연산하여 저장\n",
    "t5 = torch.sub(t1, t2) # 메소드 이용\n",
    "t6 = t1 - t2 # 연산자 이용\n",
    "\n",
    "# 결과 출력\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐세 연산: 곱셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 곱셈 연산하여 저장\n",
    "t7 = torch.mul(t1, t2) # 메소드 이용\n",
    "t8 = t1 * t2 # 연산자 이용\n",
    "\n",
    "# 결과 출력\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐세 연산: 나눗셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T10:59:58.189422Z",
     "start_time": "2023-09-10T10:59:58.105574900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 나눗셈 연산하여 저장\n",
    "t9 = torch.div(t1, t2) # 메소드 이용\n",
    "t10 = t1 / t2 # 연산자 이용\n",
    "\n",
    "# 결과 출력\n",
    "print(t9)\n",
    "print(t10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 메소드, 연산자를 이용하는 연산하는 두 방법 모두 같은 결과가 나왔다.\n",
    "- 텐서의 데이터가 각각 매칭되어 연산 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 내적, 텐서의 곱셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:00:43.151162100Z",
     "start_time": "2023-09-10T11:00:43.084675Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# torch.dot()을 사용하여 1차원 텐서의 내적을 계산하여 저장\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size()) # >>> tensor(7) torch.Size([])\n",
    "                     # 계산 결과: 0차원인 스칼라\n",
    "\n",
    "# torch.mm()을 사용하여 2차원 텐서의 곱셈을 계산히여 저장\n",
    "t2 = torch.randn(2, 3) # 2x3\n",
    "t3 = torch.randn(3, 2) # 3x2\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size()) # >>> tensor([[1.6750, 2.2840], [0.0956, 1.0294]]) torch.Size([2, 2])\n",
    "\n",
    "# torch.bmm()을 사용하여 3차원 텐서의 곱셈을 계산히여 저장\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size()) # >>> torch.Size([10, 3, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.dot()\n",
    "  : 두 1차원 텐서 간의 내적 계산. 스칼라값 반환.\n",
    "  : 두 텐서 원소 개수 일치해야함.\n",
    "- torch.mm()\n",
    "  : 두 2차원 텐서 간의 텐서의 곱셈만 계산.\n",
    "  : (n x m) by (m x p) -> (n x p)\n",
    "  : broadcasting을 지원 안함.\n",
    "- torch.bmm()\n",
    "  : 두 3차원 텐서 간의 텐서의 곱셈 계산.\n",
    "  : (b x n x m) by (b x m x p) -> (b x n x p)\n",
    "  : broadcasting을 지원 안함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 다차원 텐서 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "텐서 곱 (브로드캐스팅: 크기가 서로 다른 텐서를 자동으로 맞춰주는 기능)\n",
    "mm은 브로드캐스팅이 불가능하다는 차이점이 있음\n",
    "규칙 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:01:12.252213900Z",
     "start_time": "2023-09-10T11:01:12.142570500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "# 1차원 x 1차원 -> 스칼라 (0차원)\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "# 2차원 x 1차원 -> 1차원\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "# 3차원 x 1차원 -> 2차원\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "# 3차원 x 3차원 -> 3차원\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n",
    "# 3차원 x 2차원 -> 3차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 브로드캐스팅 기능 제공하므로 크가가 맞지 않는 연산에 대해 자동으로 크기를 맞춰서 연산을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# i_tensor_broadcasting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 연산자와 메소드를 통한 브로드캐스팅 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# 1차원과 스칼라의 곱셈 연산 \n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2) # 결과: 1차원\n",
    "               # 모든 원소에 스칼라 값 곱해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "source": [
    "# 2차원과 1차원의 뺄셈 연산 (브로드캐스팅)\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4) # 결과: 2차원\n",
    "               # 1차원 텐서가 2차원 텐서에 맞춰져 뺄셈 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 2차원과 스칼라의 사칙연산 (브로드캐스팅)\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0) \n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "# 결과: 모두 2차원\n",
    "# 스칼라가 2차원 텐서에 맞춰져 사칙연산 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 함수를 이용하여 나눗셈 연산 (브로드캐스팅)\n",
    "def normalize(x): # 인자에 대하여 스칼라와 나눗셈 연산 값 반환\n",
    "  return x / 255\n",
    "\n",
    "t6 = torch.randn(3, 28, 28) # 3차원 텐서\n",
    "print(normalize(t6).size()) # 스칼라가 3차원에 맞춰져서 나눗셈 계산 후 반환 받음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 다양한 차원 조합에 따른 브로드캐스팅 덧셈 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]]) # t8이 t7에 맞춰 연산\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]]) # t9이 t7에 맞춰 연산\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]]) # t8과 t9이 서로 맞춰 연산\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]]) # t10이 t7 맞춰 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * ones, rand, empty 메소드로 텐서 생성하여 브로드캐스팅 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# 일치하지 않는 축에 대해 값이 큰 쪽으로 맞춰서 연산\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape) # >>> torch.Size([4, 3, 2])\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape) # >>> torch.Size([4, 3, 2])\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape) # >>> torch.Size([4, 3, 2])\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size()) # >>> torch.Size([5, 3, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 일치하지 않는 축에 대해 값이 큰 쪽으로 맞춰서 연산\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "\n",
    "# 브로드캐스트 불가로 인한 오류\n",
    "# 3번째 축의 크기가 각각 2, 3으로 \n",
    "# 둘 중 하나가 1이거나 존재하지 않아야 한다는 규칙 만족 못함\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <브로드캐스트 규칙>\n",
    "- 각 텐서는 최소한 한 차원은 가지고 있다.\n",
    "- 뒤 쪽에서 시작하여 차원의 크기는 동일해야하고, 다르다면 그 중 하나는 1이거나 존재하지 않아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 스칼라 연산과 제곱 연산에 대한 브로드캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:01:37.305776800Z",
     "start_time": "2023-09-10T11:01:37.246149400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서와 스칼라의 곱셈 연산 (브로드캐스팅)\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # 1차원 결과 >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "# 1차원 텐서와 스칼라의 제곱 연산 (브로드캐스팅)\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # 1차원 결과 >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "# 1차원 텐서와 1차원 텐서의 제곱 연산 (브로드캐스팅)\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # 1차원 텐서 >>> tensor([   1.,    4.,   27.,  256.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서의 차원과 크기가 달라도 브로드캐스팅을 통하여 차원을 맞춘 후에 연산을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서의 인덱싱, 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "# 인덱싱: 첫 번째 행 선택\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "\n",
    "# 슬라이싱 및 인덱싱: 모든 행에서 두 번째 열 선택\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "\n",
    "# 인덱싱: 두 번째 행의 세 번째 열 선택\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "\n",
    "#슬라이싱 및 인덱싱: 모든 행에서 마지막 열 선택\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 다차원 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "# 슬라이싱: 두 번째 행부터 끝까지 선택\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "\n",
    "# 슬라이싱: 두 번째 행부터 끝까지, 네 번째 열부터 끝까지 선택\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 인덱싱과 슬레이싱을 이용한 값 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 번째부터 네 번째 행의 세 번째 열에 1 할당\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "# 두 번째부터 네 번째 행과 두 번째부터 네 번째 열 선택하여 출력\n",
    "print(y[1:4, 1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:01:56.681100800Z",
     "start_time": "2023-09-10T11:01:56.598186900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "\n",
    "# 첫 두 행 선택\n",
    "print(z[:2])\n",
    "\n",
    "# 두 번째부터 끝까지 행과 두 번째부터 세 번째 열 선택\n",
    "print(z[1:, 1:3])\n",
    "\n",
    "# 모든 행과 두 번째부터 끝까지 열 선택\n",
    "print(z[:, 1:])\n",
    "\n",
    "# 두 번째부터 끝까지 행과 두 번째부터 세 번째 열의 값에 0 할당\n",
    "z[1:, 1:3] = 0\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 재구성: 텐서 모양 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# (2,3) 텐서 생성\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# view 메소드로 (3,2) 텐서로 변경\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "\n",
    "# reshape 메소드로 (1,6) 텐서로 변경\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "\n",
    "# 출력: 텐서 모양이 변경되었음\n",
    "print(t2) # (3,2)\n",
    "print(t3) # (1, 6)\n",
    "\n",
    "# view의 텐서 모양 변경을 이용하여 텐서 생성\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 차원 추가 및 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# 차원 삭제 (squeeze 메소드)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "# 3차원 텐서 생성\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "# 크기가 1인 모든 차원 삭제\n",
    "t7 = t6.squeeze()  # Shape becomes (3,) \n",
    "\n",
    "# Remove dimension at position 0\n",
    "# 특정 위치의 차원 삭제\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1) \n",
    "\n",
    "# 차원이 삭제된 텐서 출력\n",
    "print(t7) #1, 3차원 제거 됨\n",
    "print(t8) # 3차원 제거 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 차원 추가 (unsqueeze 메소드) \n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "# 1차원 텐서 생성 \n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "# 지정된 위치에 차원 크기가 1인 차원 추가\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10) # 1차원에 추가 됨\n",
    "\n",
    "# 2차원 텐서 생성\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "\n",
    "# 지정된 위치에 차원 크기가 1인 차원 추가\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3) \n",
    "print(t12, t12.shape) # 2차원에 추가됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 텐서 펼치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "# 텐서를 1차원으로 평면화\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "# 결과 출력: 1차원 텐서\n",
    "print(t14) \n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "\n",
    "# 1차원으로 평면화\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "# 두 번째 차원부터 평면화\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(t16) # 1차원 텐서\n",
    "print(t17) # 2차원 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 차원 순서 변경 (Permute와 Transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:02:41.367314500Z",
     "start_time": "2023-09-10T11:02:41.291466100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "\n",
    "# 차원 순서 변경\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "# 텐서 생성\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "# 차원 순서 변경\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "\n",
    "# 결과 출력: 각 차원의 크기는 변하지 않았지만, 순서는 변경 됨\n",
    "print(t20) \n",
    "print(t21) \n",
    "\n",
    "# Transpose the tensor\n",
    "# 텐서 전치\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "# 결과 출력: 각 차원의 크기는 변하지 않았지만, 순서는 변경 됨\n",
    "print(t22)\n",
    "\n",
    "# t메소드도 transpose 하는 역할을 함\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "# 결과 출력: transpose 메소드에 의한 결과와 동일\n",
    "print(t23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * cat 으로 텐서 병합 (차원 유지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 텐서 생성\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "# 여러 텐서를 연결하여 새로운 텐서 생성\n",
    "t4 = torch.cat([t1, t2, t3], dim=1) # 두번째 차원을 따라 연결\n",
    "print(t4.shape) # 두 번째"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서 2개 생성\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "# 첫번째 차원을 따라 텐서를 병합하여 새로운 텐서 생성\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # 1차원에 모든 데이터 들어감 >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# 2차원 텐서 2개 생성\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합 (두 번째 차원을 따라)\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "# 2차원 텐서간 병합 (첫 번째 차원을 따라)\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "# 2차원 텐서 3개 생성\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 3개의 2차원 텐서 병합 (두 번째 차원을 따라 )\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "# 3개의 2차원 텐서 병합 (첫 번째 차원을 따라 )\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:05:58.732286600Z",
     "start_time": "2023-09-10T11:05:58.659155900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 텐서 생성\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "# 3차원 텐서 병합 (세 번째 차원을 따라)\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "# 3차원 텐서 병합 (두 번째 차원을 따라)\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "# 3차원 텐서 병합 (첫 번째 차원을 따라)\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * stack을 이용한 병합 (차원 추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:06:17.539947100Z",
     "start_time": "2023-09-10T11:06:17.458095200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "# 2개의 2차원 텐서 생성\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# 1\n",
    "# stack으로 텐서 병합 (세 번째 차원을 추가하며)\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "\n",
    "# cat으로 stack 효과 내기 (세 번째 차원 추가하고, 세 번째 차원을 따라 병합)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "\n",
    "# 두 메소드로 병합된 텐서 출력 및 동등성 확인\n",
    "print(t3.shape, t3.equal(t4)) # 일치함\n",
    "\n",
    "\n",
    "# 2\n",
    "# stack으로 텐서 병합 (두 번째 차원을 추가하며)\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "\n",
    "# cat으로 stack 효과 내기 (두 번째 차원 추가하고, 두 번째 차원을 따라 병합)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "# 두 메소드로 병합된 텐서 출력 및 동등성 확인\n",
    "print(t5.shape, t5.equal(t6)) # 일치함\n",
    "\n",
    "\n",
    "# 3\n",
    "# stack으로 텐서 병합 (첫 번째 차원을 추가하며)\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "\n",
    "# cat으로 stack 효과 내기 (첫 번째 차원 추가하고, 첫 번째 차원을 따라 병합)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "\n",
    "# 두 메소드로 병합된 텐서 출력 및 동등성 확인\n",
    "print(t7.shape, t7.equal(t8)) # 일치함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서 2개 생성\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "# 1\n",
    "# stack으로 텐서 병합 (두 번째 차원을 추가하며)\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "# cat으로 stack 효과 내기 (두 번째 차원 추가하고, 두 번째 차원을 따라 병합)\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "\n",
    "#2\n",
    "# stack으로 텐서 병합 (첫 번째 차원을 추가하며)\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "\n",
    "# cat으로 stack 효과 내기 (첫 번째 차원 추가하고, 첫 번째 차원을 따라 병합)\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 같은 사이즈의 텐서끼리만 쌓을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * vstack을 이용하여 텐서 병합 (수직으로 스택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-10T11:06:39.677668600Z",
     "start_time": "2023-09-10T11:06:39.592351600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서 생성\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# vstack으로 텐서 병합 (수직으로 스택)\n",
    "t3 = torch.vstack((t1, t2)) # 튜플로 두 텐서 묶어서 전달하는 방식\n",
    "\n",
    "# 출력 결과: 2차원 텐서, 두 번째 차원 2로 추가 됨\n",
    "print(t3) \n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "# 2차원 텐서 생성\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "\n",
    "# vstack으로 텐서 병합 (수직으로 스택)\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# 출력 결과: 2차원 텐서, 두 번째 차원 크기 증가\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "# 3차원 텐서 생성\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# vstack으로 텐서 병합 (수직으로 스택)\n",
    "t9 = torch.vstack([t7, t8]) # 리스트로 두 텐서 묶어서 전달하는 방식\n",
    "\n",
    "# 출력 결과 : 3차원 텐서, 세 번째 차원의 크기 증가\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * hstack을 이용하여 텐서 병합 (수팽으로 스택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서 생성\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# hstack으로 텐서 합병 (수평으로 스택)\n",
    "t12 = torch.hstack((t10, t11))\n",
    "\n",
    "# 출력 결과: 1차원 텐서, 첫 번째 차원 크기 증가\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "\n",
    "# 2차원 텐서 생성\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "\n",
    "# hstack으로 텐서 합병 (수평으로 스택)\n",
    "t15 = torch.hstack((t13, t14))\n",
    "\n",
    "# 출력 결과: 2차원 텐서, 첫 번째 차원 크기 증가\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "# 3차원 텐서 생성\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# hstack으로 텐서 합병 (수평으로 스택)\n",
    "t18 = torch.hstack([t16, t17])\n",
    "\n",
    "# 출력 결과: 3차원 텐서, 두 번째 차원 크기 증가\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " 과제에서 복잡한 데이터를 효율적으로 관리할 수 있는 tenser의 기능을 자세히 알게되면서 텐서가 딥러닝 작업에 필수적이라는 것을 알게 되었다. 처음에는 NumPy 배열로 충분하지 않은가 의문을 갖기도 했지만, GPU를 사용이나 다양한 연산 등을 tenser에서 지원하기 때문에 그 필요성을 깨달을 수 있었다.\n",
    "\n",
    " \n",
    " 그리고 tenser를 반환할 때 깊은 복사인지 얕은 복사인지 신경쓰는 것과 자주 shape나 size 같은 tenser의 정보를 확인해 보는 것이 텐서를 사용할 때 중요하다는 점도 배울 수 있었다. 데이터의 차원이 커질 수록 그 형태를 상상하기 어려워 지는데, tenser의 정보를 출력하여 확인하면 그 형태를 생각하고 관리하는데 큰 도움이 된다는 것도 느낄 수 있었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
